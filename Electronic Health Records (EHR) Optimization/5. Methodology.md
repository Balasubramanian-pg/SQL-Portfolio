# Methodology Document

## Electronic Health Records (EHR) Optimization â€” Data Layer Methodology

### Executive summary

This methodology explains, step by step, how to redesign and optimize an Electronic Health Records database so that patient lookups are fast, correct, and predictable as data grows. It covers principles, phased delivery, modeling patterns, ingestion and validation, Snowflake-specific implementation guidance, testing, benchmarking, rollout, monitoring, and maintenance. The target audience is data engineers, database architects, and engineering leads who will implement the data foundation in a cloud data platform.

Key load-bearing facts used in platform decisions:

* Snowflake relies on micro-partitioning and supports clustering keys rather than traditional B-tree indexes. ([Snowflake Documentation][1])
* Electronic health information must be protected for confidentiality, integrity, and availability under HIPAA frameworks. ([HHS.gov][2])
* FHIR is the de facto interoperability standard for exchanging clinical data between systems. ([hl7.org][3])
* EHR performance and workflow design materially affect clinician time and some measures of care quality. ([PMC][4])

## Table of contents

1. Principles and design goals
2. Project phases and deliverables
3. Data modeling methodology
4. Table design patterns and conventions
5. Ingestion, staging, and canonicalization
6. Referential integrity and constraints
7. Query patterns and SQL conventions
8. Snowflake-specific implementation patterns
9. Validation, testing, and QA strategy
10. Performance benchmarking and profiling
11. Rollout, deployment, and cutover plan
12. Monitoring, observability, and maintenance
13. Security, governance, and compliance notes
14. Risk register and mitigation
15. Resource, timeline, and deliverables matrix
16. Appendix: example SQL snippets, test cases, and checklists
17. Points requiring verification or uncertain items

## 1 Principles and design goals

### Core principle

Design the data foundation around how clinicians query and reason about patients: patient first, events second, time always present. Data structure must reflect human access patterns. When structure and access align, SQL is simple, predictable, and fast.

### Measurable goals

* Median patient retrieval latency under target SLO (e.g., 200 ms for single patient view under normal concurrency) - set project-specific SLOs during planning.
* Referential integrity 100 percent for production data sets - no orphan clinical events allowed.
* Predictable performance across growth horizons; plan for 3 to 5 year data retention without schema redesign.
* Query complexity reduction: target 30 to 50 percent fewer complex defensive joins in operational queries.

### Nonfunctional priorities

* Data correctness over clever but fragile performance hacks.
* Platform-aware design not platform-dependent code.
* Operational observability for query health and schema drift.
* Auditability for every record and change.

## 2 Project phases and deliverables

### Phase 0 - Initiation and discovery (1-2 weeks)

Deliverables:

* Stakeholder map and interview notes
* Inventory of current schemas, top 50 queries, and slowest queries
* Data volumes and growth rates by table
* Source systems and ingestion contracts

Activities:

* Interview clinicians, clinical informaticists, and application owners to collect top queries and pain points.
* Pull query history from Snowflake or source DB to identify hotspots.
* Collect sample data volumes and identify retention policies.

### Phase 1 - Model design and canonical schema (2-3 weeks)

Deliverables:

* Canonical data model (ER diagrams) anchored on Patient
* Table-level definitions with data types, cardinalities, and SCD decisions
* Onboarding rules for provider and reference vocabularies

Activities:

* Define primary keys, surrogate keys, and stable natural keys.
* Decide event table grain and time columns.
* Define acceptable controlled vocabularies for statuses and code sets.

### Phase 2 - Ingestion and staging pipelines (2-4 weeks)

Deliverables:

* Staging schema definitions
* ETL/ELT mapping specifications
* Data validation rules and rejection handling

Activities:

* Implement idempotent ingestion patterns
* Canonicalize incoming identifiers
* Build early checks to reject orphaned events

### Phase 3 - Referential integrity and constraints (1-2 weeks)

Deliverables:

* FK enforcement plan (logical or physical as supported)
* Scripts or processes to backfill integrity for legacy data
* Audit tables for data corrections

Activities:

* Build join-time checks and blocking checks for ingestion
* Create reconciliation jobs between staging and canonical tables

### Phase 4 - Query library and optimization (2-3 weeks)

Deliverables:

* Library of optimized patient-centric queries and UDFs if needed
* Query templates for common clinical screens and reports
* Guidance on column projection and predicate usage

Activities:

* Replace wide SELECT * patterns with narrow projections
* Create materialized or cached patient views when justified

### Phase 5 - Performance tuning and benchmarking (2-4 weeks)

Deliverables:

* Benchmark results across representative workloads
* Clustering key and distribution recommendations
* Query plan analysis reports and recommendations

Activities:

* Simulate peak concurrency
* Track scanned micro-partitions and query profile metrics
* Iteratively adjust clustering and data layout

### Phase 6 - Validation, acceptance, and rollout (2-3 weeks)

Deliverables:

* Acceptance test suite results
* Rollout plan and cutover checklist
* Post-deployment performance guardrails

Activities:

* Run integration and user acceptance tests with anonymized data
* Execute phased rollout
* Monitor and revert plan rehearsals

### Phase 7 - Handover and maintenance (ongoing)

Deliverables:

* Runbook and operating playbooks
* Monitoring dashboards and alerts
* Periodic review calendar for cluster keys and archived partitions

Activities:

* Establish monthly performance reviews
* Automate reclustering or maintenance jobs as needed

## 3 Data modeling methodology

### Anchor concept: patient-centric design

* Single authoritative Patients table with a stable, immutable PatientKey.
* All clinical tables are event driven and reference PatientKey.
* Time columns included on every event table: event_timestamp, created_at, updated_at.

Rationale:

* Most clinical queries start with a patient identifier.
* Time series is primary; clinicians view patient history chronologically.
* Event modeling reduces ambiguity between state and historical event.

### Table categories

* Dimensions and references: Patients, Providers, Locations, CodeSystems.
* Event tables: Appointments, Prescriptions, LabResults, MedicalRecords, Orders.
* Audit and lineage: IngestLogs, ChangeLog, CorrectionEvents.

### Keys and identity

* Use a surrogate integer or UUID as the canonical PatientKey to avoid natural key reuse issues.
* Keep natural identifiers (national id, MRN) in the Patients table as attributes for mapping.
* ProviderKey behaves similarly; keep external IDs for integration.

### Handling mutable attributes

* For demographic attributes that can change, use slowly changing dimension pattern 2 for auditability or keep latest attributes with a history table when required.
* Decision matrix:

  * If clinical decisions require historical demographic fidelity, implement SCD2.
  * If not, store current attributes and write change events to audit table.

### Event table grain

Define grain carefully. Examples:

* Appointments: one row per scheduled visit instance.
* MedicalRecords: one row per clinical note or record item.
* LabResults: one row per performed test component.

Avoid multi-event rows (for example a single row that stores multiple lab panels as JSON) unless there is a very specific performance or storage reason.

### Normalization vs. denormalization

* Normalize identifiers and small reference sets to reduce storage and improve data quality.
* Denormalize read-heavy attributes into event rows when it reduces joins and improves operational read latency, but do so intentionally and document the trade-offs.

## 4 Table design patterns and conventions

### Naming conventions

* Schema: ehr_core, ehr_staging, ehr_audit
* Tables: ehr_core.patients, ehr_core.appointments, ehr_core.prescriptions
* Columns: patient_key, provider_key, event_timestamp, created_at, updated_at

### Column type and precision

* Use DATE for dates, TIMESTAMP_NTZ for event timestamps if microsecond precision not required. Choose TIMESTAMP_TZ only when cross-timezone semantics are necessary and verified.
* Use fixed-length types for codes when possible to minimize compression variance.

### Constraints and nullability

* Primary keys not nullable.
* Foreign keys logically enforced; Snowflake supports specifying constraints but they may be non-enforced in older setups. Enforce referential integrity through pipelines and validation if physical FK enforcement is not available or enabled.

### Partitioning strategy and time

* Logical partitions will reflect time. In Snowflake, micro-partitions are automatic. Choose clustering keys that align with the most frequent predicates, typically patient_key and event_timestamp for event tables. ([Snowflake Documentation][1])

### Compression and storage

* Keep columns narrow and use consistent data types to take advantage of columnar compression.
* Avoid storing large free text in the same table as frequently filtered fields. Put large textual blobs in separate columns or even an object store reference if needed.

## 5 Ingestion, staging, and canonicalization

### Staging area and contracts

* All incoming feeds land in ehr_staging.<source> with raw payload preserved.
* Maintain provenance columns: source_system, source_record_id, ingest_batch_id, ingest_timestamp.

### Idempotent ingestion

* Upserts must be idempotent. Use deterministic dedupe keys built from source ids and timestamps.
* For incremental loads, prefer merge patterns that upsert based on source keys and source last modified timestamps.

### Canonicalization

* Map source identifiers to canonical keys before writing to core tables.
* Clean and normalize code sets on ingest; maintain a CodeSystem table for lookups.

### Rejection and quarantining

* Invalid rows go to ehr_audit.rejected_rows with error codes and payload.
* Set SLAs for human review of rejection queues.

### Handling late-arriving data

* Late data must be accepted and reconciled with audit logs. Design reconciliation jobs that can apply corrections and track lineage.

## 6 Referential integrity and constraints

### Enforcement options

* Where platform supports physical FK enforcement, enable it. If not available or costly, implement enforcement via pipeline checks and nightly validation jobs.
* Always maintain audit trails for enforced or corrected integrity failures.

### Backfill strategy for legacy data

* Run discovery queries to find orphaned child rows.
* Create correction jobs that either map to an existing patient key or quarantine the records for manual resolution.

### Id mapping lifecycle

* Keep an id_map table for mapping external identifiers to canonical keys, with version history and effective dates.

## 7 Query patterns and SQL conventions

### Guiding rules

* Prefer narrow projections over SELECT * in production queries.
* Always filter by patient_key for operational queries.
* Use LEFT JOINs to preserve patient rows when related data may be missing.
* Avoid correlated subqueries when a well indexed join provides the same result at lower cost.

### Common templates

* Patient snapshot:

SELECT p.patient_key, p.first_name, p.last_name, p.date_of_birth, a.appointment_date, pres.medication
FROM ehr_core.patients p
LEFT JOIN (
SELECT * FROM ehr_core.appointments WHERE event_timestamp >= DATEADD(year, -1, current_date)
) a ON p.patient_key = a.patient_key
LEFT JOIN ehr_core.prescriptions pres ON p.patient_key = pres.patient_key
WHERE p.patient_key = :patient_key;

Explainable reasons for patterns:

* Subselects limit scanned micro-partitions when date window is used.
* LEFT JOIN ensures missing data does not drop the patient.

### Materialized patient views vs. dynamic queries

* Create materialized or cached patient views only when:

  * Query frequency and latency justify it
  * The cost of maintaining the view is less than repeated query cost
* Prefer lightweight materialized views for high frequency low latency screens.

### Pagination and chunking

* For large longitudinal exports, implement keyset pagination on event_timestamp and event_id.
* Avoid OFFSET-LIMIT style pagination for large offsets.

## 8 Snowflake-specific implementation patterns

### Micro-partitioning and clustering

* Snowflake micro-partitions are automatic. To help pruning, choose clustering keys that match WHERE predicates such as patient_key and event_timestamp. Use clustering sparingly and monitor benefits using SYSTEM$CLUSTERING_INFORMATION. ([Snowflake Documentation][1])

### Avoid relying on traditional index constructs

* Do not design for B-tree indexes. Snowflake does not use classic indexes. Clustering keys and selective predicates are the right tools. ([Snowflake Documentation][5])

### Data types and compression

* Use consistent data types to maximize column compression. Avoid schema drift where possible.

### Copy into and staged loads

* Use Snowflake stages for loading large batches and COPY INTO for performant ingestion. Where continuous ingestion is required, use Snowpipe or streaming ingestion patterns.

### Cost-conscious queries

* Narrow projections and predicate push-down reduce scanned data and therefore cost.
* Use QUERY_HISTORY metrics to monitor scanned bytes and adjust queries and cluster keys.

### Clustering maintenance

* Monitor clustering depth and automatic clustering costs. Use explicit RECLUSTER or run maintenance on a schedule if necessary, but prefer workload-driven adjustments. ([Snowflake Documentation][5])

## 9 Validation, testing, and QA strategy

### Testing pyramid

* Unit tests for SQL building blocks
* Integration tests for join correctness
* End-to-end tests for clinical scenarios

### Test data and anonymization

* Use synthetic data that mirrors production distributions.
* If production data is used for testing, apply robust anonymization and comply with legal and policy requirements.

### Test cases

* Referential integrity tests: ensure every child row maps to a valid parent.
* De-duplication tests: ensure no duplicate patient or event keys exist.
* Temporal order tests: event timestamps are in expected sequence for a patient.
* Missing data resilience: queries still return patient rows when related tables are empty.

### Acceptance criteria

* All acceptance tests must pass in staging with performance metrics within SLA.
* No critical referential violations remain unresolved.

## 10 Performance benchmarking and profiling

### Representative workloads

* Create realistic simulated workloads:

  * Clinic peak load: high-concurrency small patient-view queries
  * Reporting load: large scans and aggregations
  * Mixed load: overlapping operational and reporting

### Metrics to collect

* Query latency distribution: p50, p90, p99
* Bytes scanned per query
* CPU / credit consumption
* Concurrency and queue times

### Profiling tools

* Use Snowflake Query Profile and Query History to inspect scanned micro-partitions and operator metrics. ([Snowflake Documentation][1])

### Benchmarking approach

* Baseline: measure current performance on identified critical queries.
* Iterative changes: apply one change and retest to measure impact.
* Report: create before-and-after dashboards for latency, scanned bytes, and compute cost.

### Acceptable trade-offs

* If reducing latency by X percent increases cost by Y percent, document the trade-off and get stakeholder approval. Provide cost per 1 ms improvement when possible.

## 11 Rollout, deployment, and cutover plan

### Phased rollout

* Pilot group: route a small subset of application reads to the new schema and queries.
* Partial rollout: expand to a set of clinics or departments.
* Full rollout: switch application reads for all users.

### Rollback plan

* Maintain the old read path active and switch traffic via a feature flag or a router.
* Keep incremental syncs between old and new schema until cutover is final.

### Communication and training

* Notify clinicians of changes and expected improvements.
* Provide runbooks for application owners and support teams.

### Post-cutover monitoring

* Keep a 72-hour high-attention window for dashboard monitoring and quick rollback capability.
* Implement post-cutover validation jobs to compare old and new read path results.

## 12 Monitoring, observability, and maintenance

### Dashboards to build

* Query latency heatmap and top slow queries
* Bytes scanned per query and cost per query
* Referential integrity daily report
* Reconciliation queue size and aging

### Alerts

* Alert on p99 patient lookup latency exceeding SLA
* Alert on unusual increases in bytes scanned for common queries
* Alert on growth in rejected ingestion rows

### Maintenance cadence

* Monthly performance review focused on clustering and query patterns
* Quarterly data model review for new clinical requirements
* Annual archival and retention review

### Runbooks

* Incident runbook for slow patient lookup during peak hours
* Reconciliation runbook for orphaned events
* Data correction playbook with audit requirements

## 13 Security, governance, and compliance notes

### Data protection baseline

* Ensure encryption at rest and in transit via platform defaults and additional controls where required.
* Implement least privilege access controls around schemas and tables.

### Regulatory posture

* The project needs to treat ePHI according to applicable regulations. HIPAA requires administrative, physical, and technical safeguards for ePHI. Organize documentation and controls accordingly. ([HHS.gov][2])

### Data access and auditing

* Implement role-based views and mask sensitive fields where appropriate.
* Maintain audit logs for changes to reference and patient records.

### Interoperability caution

* This project excludes implementing FHIR APIs but documents canonical mappings to FHIR resources for downstream teams. FHIR is the common standard for data exchange and should guide mapping decisions. ([hl7.org][3])

## 14 Risk register and mitigation

### Risk: Orphaned legacy data

* Impact: High
* Mitigation: Large-scale discovery queries, quarantining, staged human reconciliation.

### Risk: Query regressions after schema changes

* Impact: High
* Mitigation: Maintain the old read path during rollout, run differential tests, and use blue-green switch.

### Risk: Unexpected cost increase due to clustering or materialized views

* Impact: Medium
* Mitigation: Cost modeling for each design change, approval gates, and revert plan.

### Risk: Regulatory noncompliance

* Impact: High
* Mitigation: Engage legal and compliance early, apply strict access controls, log everything.

### Risk: Data drift and source schema changes

* Impact: Medium
* Mitigation: Contract data contracts with sources and automated schema drift detection.

## 15 Resource, timeline, and deliverables matrix

### Typical small team (example)

* 1 Data Architect (lead)
* 2 Data Engineers (implementation)
* 1 Clinical informaticist (domain expert)
* 1 QA engineer
* 1 Platform engineer (Snowflake ops)

### Example timeline (12-16 weeks typical)

* Weeks 1-2: Discovery and inventory
* Weeks 3-5: Model design and canonical schema
* Weeks 6-8: Ingestion and staging implementation
* Weeks 9-10: Referential integrity and backfill
* Weeks 11-12: Query optimization and materialization
* Weeks 13-14: Benchmarking and acceptance
* Weeks 15-16: Pilot rollout and stabilization

### Deliverables list

* ERDs and canonical schema documentation
* Ingestion mapping and staging specs
* Query library with templates
* Benchmarking report and cluster key recommendations
* Acceptance test suites and results
* Runbooks and maintenance playbooks

## 16 Appendix: example SQL snippets, test cases, and checklists

### Example canonical Patients table DDL (Snowflake-friendly)

CREATE TABLE ehr_core.patients (
patient_key NUMBER AUTOINCREMENT PRIMARY KEY,
external_mrn VARCHAR(100) NOT NULL,
first_name VARCHAR(200),
last_name VARCHAR(200),
date_of_birth DATE,
gender VARCHAR(50),
address VARCHAR(500),
phone VARCHAR(50),
email VARCHAR(200),
created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);

Notes:

* Choose AUTOINCREMENT or SEQUENCE patterns consistent with team standards.
* Keep natural keys in columns for mapping.

### Example clustering recommendation

For event-heavy tables such as lab_results and medical_records, set a clustering key of (patient_key, event_timestamp) when queries typically filter by patient and date ranges. Monitor using SYSTEM$CLUSTERING_INFORMATION. ([Snowflake Documentation][6])

### Example ingestion merge (pseudo)

MERGE INTO ehr_core.lab_results t
USING staging.lab_results s
ON t.source_system = s.source_system AND t.source_record_id = s.source_record_id
WHEN MATCHED AND s.is_deleted = 'Y' THEN DELETE
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT (...);

### Test case examples

* TC001 Referential Integrity: After full load, COUNT(child rows without parent) = 0.
* TC002 Snapshot Consistency: For N selected patients, new patient snapshot equals application expected fields.
* TC003 Performance: Median patient query latency <= SLA under simulated concurrency.

### Checklist before production cutover

* All critical queries pass performance SLAs
* No unresolved referential violations older than threshold
* Reconciliation jobs run successfully for full-day windows
* Runbooks reviewed and communication plan executed

## 17 Points requiring verification or uncertain items

* Confirm whether physical FK enforcement is enabled on the Snowflake account and what enforcement semantics apply. If constraints are not enforced by platform settings, pipeline-level enforcement is required. ([Snowflake Documentation][1])
* Decide the exact SLO numbers for patient lookup latency and acceptable cost trade-offs. These are organizational decisions and must be provided by stakeholders.
* Confirm retention policy and legal requirements for clinical data in your jurisdiction, because retention impacts storage planning and archival cost. Regulatory guidance varies; consult legal. ([HHS.gov][2])
* Validate that all source systems can provide consistent and unique source_record_id values. If not, define a robust deduplication and mapping strategy.
* If FHIR support is required in the future, confirm which FHIR profiles or implementation guides to map to. HL7 and local health systems may have region-specific profiles. ([hl7.org][3])

## Closing notes and next steps

### Immediate next steps

* Schedule discovery interviews with clinical and application stakeholders to capture the top 25 patient queries.
* Pull query history and data volume metrics for the top tables.
* Create a minimal staging environment with synthetic data to validate model assumptions.

### What this methodology ensures

* A clear, patient-centric canonical model
* Platform-aware implementation choices that avoid brittle constructs
* A repeatable, testable process for rolling out performance improvements
* Operational visibility and maintenance cadence for long-term success

If you want, I will now:

* Produce the ERD and table DDL set for the canonical schema described above
* Create the patient query library with parameterized templates and test harness
* Produce the benchmark script and synthetic data generator for your expected data volumes

Tell me which deliverable you want first and I will produce it in the next message.

[1]: https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions?utm_source=chatgpt.com "Micro-partitions & Data Clustering"
[2]: https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html?utm_source=chatgpt.com "Summary of the HIPAA Security Rule"
[3]: https://www.hl7.org/fhir/overview.html?utm_source=chatgpt.com "HL7 FHIR Standard Overview"
[4]: https://pmc.ncbi.nlm.nih.gov/articles/PMC1205599/?utm_source=chatgpt.com "The Impact of Electronic Health Records on Time Efficiency of ..."
[5]: https://docs.snowflake.com/en/user-guide/tables-clustering-keys?utm_source=chatgpt.com "Clustering Keys & Clustered Tables"
[6]: https://docs.snowflake.com/en/sql-reference/functions/system_clustering_information?utm_source=chatgpt.com "SYSTEM$CLUSTERING_INFORMATION"

