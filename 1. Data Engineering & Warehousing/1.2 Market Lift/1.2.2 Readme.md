# Market Lift SQL Data Warehouse Documents

This file contains separate ready-to-copy documents for each pending SQL data warehousing component you requested. Each section is written as a standalone document you can paste into design docs, tickets, or implementation scripts.

# Document 1: Explicit Grain Enforcement

## Purpose

Provide a definitive grain contract for the fact table and SQL implementations that enforce, validate, and monitor the grain.

## Grain Statement

One row per product, per location, per date, per campaign. If no campaign influences the row, CampaignID will be 0 or NULL depending on platform standards.

## Why this matters

* Prevents silent row multiplication when joining
* Ensures aggregations are correct
* Enables reliable uniqueness checks

## Implementation Checklist

* Add a surrogate FactID or a composite unique key
* Implement a unique constraint or a materialized validation check
* Create monitoring queries that detect duplicates

## Example DDL (Snowflake / ANSI SQL)

```sql
CREATE TABLE fact_market_sales (
  fact_id BIGINT AUTOINCREMENT PRIMARY KEY,
  date_key INT NOT NULL,
  location_id INT NOT NULL,
  product_id INT NOT NULL,
  campaign_id INT,
  baseline_units INT NOT NULL,
  actual_units INT NOT NULL,
  lift_units INT GENERATED ALWAYS AS (actual_units - baseline_units),
  revenue_usd NUMERIC(18,2) NOT NULL,
  load_ts TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE UNIQUE INDEX uq_fact_market_sales_grain
  ON fact_market_sales (date_key, location_id, product_id, COALESCE(campaign_id, 0));
```

Notes

* If your platform does not support unique indexes on large fact tables, implement a nightly dedupe and report.

## Validation Queries

Detect duplicates

```sql
SELECT date_key, location_id, product_id, COALESCE(campaign_id, 0) AS campaign_key,
       COUNT(*) AS row_count
FROM fact_market_sales
GROUP BY 1,2,3,4
HAVING COUNT(*) > 1;
```

Enforce via stage-to-produce pattern

```sql
-- 1. Load into staging table
-- 2. Deduplicate by keeping latest load_ts per grain
MERGE INTO fact_market_sales tgt
USING (
  SELECT date_key, location_id, product_id, COALESCE(campaign_id, 0) AS campaign_key,
         MAX(load_ts) AS max_ts
  FROM staging_market_sales
  GROUP BY 1,2,3,4
) src
ON tgt.date_key = src.date_key
   AND tgt.location_id = src.location_id
   AND tgt.product_id = src.product_id
   AND COALESCE(tgt.campaign_id,0) = src.campaign_key
WHEN MATCHED THEN
  UPDATE SET ...
WHEN NOT MATCHED THEN
  INSERT (...);
```

## Monitoring

* Daily job that runs the duplicate detection query and writes exceptions to a table
* Alert when duplicates appear for more than 5 unique grains in a day

## Deliverables

* DDL for fact table with uniqueness enforcement
* Dedup MERGE template
* Daily monitoring SQL and alert rule

# Document 2: Full Dim_Date

## Purpose

Provide a production-ready date dimension with calendar and fiscal attributes to avoid runtime calculations in reports.

## Attributes to include

* date_key INT in YYYYMMDD format
* date DATE
* day_of_week INT and name
* week_of_year INT
* iso_week INT
* month INT and name
* quarter INT
* year INT
* fiscal_month, fiscal_quarter, fiscal_year
* is_weekend BOOLEAN
* is_month_start BOOLEAN
* is_month_end BOOLEAN
* day_of_year INT
* business_day_flag BOOLEAN

## Example DDL and Population (generic SQL)

```sql
CREATE TABLE dim_date (
  date_key INT PRIMARY KEY,
  date DATE NOT NULL,
  day_of_week INT NOT NULL,
  day_name VARCHAR(10),
  day_of_month INT,
  day_of_year INT,
  week_of_year INT,
  month INT,
  month_name VARCHAR(15),
  quarter INT,
  year INT,
  fiscal_year INT,
  fiscal_quarter INT,
  fiscal_month INT,
  is_weekend BOOLEAN,
  is_month_start BOOLEAN,
  is_month_end BOOLEAN,
  business_day_flag BOOLEAN
);
```

Population pattern (example for Snowflake or PostgreSQL)

```sql
WITH seq AS (
  SELECT DATE '2022-01-01' + SEQUENCE(0, 3650) AS d
)
INSERT INTO dim_date
SELECT
  TO_NUMBER(TO_CHAR(d, 'YYYYMMDD')) AS date_key,
  d AS date,
  EXTRACT(DOW FROM d) + 1 AS day_of_week,
  TO_CHAR(d, 'Day') AS day_name,
  EXTRACT(DAY FROM d) AS day_of_month,
  EXTRACT(DOY FROM d) AS day_of_year,
  EXTRACT(WEEK FROM d) AS week_of_year,
  EXTRACT(MONTH FROM d) AS month,
  TO_CHAR(d, 'Month') AS month_name,
  EXTRACT(QUARTER FROM d) AS quarter,
  EXTRACT(YEAR FROM d) AS year,
  -- fiscal logic example: fiscal year starts in April
  CASE WHEN EXTRACT(MONTH FROM d) >= 4 THEN EXTRACT(YEAR FROM d) ELSE EXTRACT(YEAR FROM d) - 1 END AS fiscal_year,
  -- fiscal quarter and month derive from fiscal_year start
  -- business day flag: exclude weekends, additional holiday calendar join may be needed
  CASE WHEN EXTRACT(DOW FROM d) IN (0,6) THEN FALSE ELSE TRUE END AS business_day_flag
FROM seq;
```

## Special considerations

* Maintain a holiday table and join to set business_day_flag correctly
* Ensure fiscal definitions are parameterized and version controlled

## Deliverables

* dim_date DDL
* Population script for a configurable date range
* Holiday table template and join logic


# Document 3: Slowly Changing Dimension Strategies (SCD)

## Purpose

Define SCD strategy for each dimension and provide implementation patterns.

## Recommended SCD mapping

* Dim_Product: Type 2 for history preservation
* Dim_Location: Type 1 for corrections, Type 2 if market boundaries change frequently
* Dim_Campaign: Type 1 with audit trail; campaigns represent events, not slowly changing attributes

## SCD Type 2 Pattern

Columns to add

* surrogate_key INT PRIMARY KEY
* natural_key (product_code)
* effective_from DATE
* effective_to DATE or infinite date value
* is_current BOOLEAN
* version INT

Example DDL

```sql
CREATE TABLE dim_product (
  product_sk BIGINT AUTOINCREMENT PRIMARY KEY,
  product_id INT NOT NULL, -- natural key
  brand_name VARCHAR(255),
  therapeutic_area VARCHAR(100),
  price NUMERIC(18,2),
  effective_from DATE NOT NULL,
  effective_to DATE,
  is_current BOOLEAN DEFAULT TRUE,
  version INT DEFAULT 1
);
```

Upsert pattern

```sql
-- when new load arrives
MERGE INTO dim_product tgt
USING staging_product src
ON tgt.product_id = src.product_id
AND tgt.is_current = TRUE
WHEN MATCHED AND (tgt.brand_name <> src.brand_name OR tgt.price <> src.price) THEN
  -- close existing record
  UPDATE SET is_current = FALSE, effective_to = CURRENT_DATE, version = tgt.version
WHEN NOT MATCHED THEN
  INSERT (product_id, brand_name, therapeutic_area, price, effective_from, is_current, version)
  VALUES (src.product_id, src.brand_name, src.therapeutic_area, src.price, CURRENT_DATE, TRUE, 1);
```

## SCD Type 1 Pattern

* Overwrite current row for non-historical corrections
* Useful for location names or minor corrections

## Deliverables

* SCD decisions table listing dimension and chosen type
* Upsert MERGE templates for Type 2 and Type 1
* Backfill strategy for historical corrections


# Document 4: Baseline Auditability

## Purpose

Make baseline calculations transparent, versioned, and auditable at the SQL layer.

## Principles

* Baseline must be recomputable from inputs
* Each baseline computation must carry a version identifier and metadata
* Baseline and baseline version must be persisted in the warehouse

## Data model patterns

Option A: Persist baseline values in fact table with version fields

* baseline_units
* baseline_method_id
* baseline_version
* baseline_run_ts

Option B: Store baseline in separate fact_baseline table and join to sales fact

Example DDL for separate table

```sql
CREATE TABLE fact_baseline (
  baseline_id BIGINT AUTOINCREMENT PRIMARY KEY,
  date_key INT NOT NULL,
  location_id INT NOT NULL,
  product_id INT NOT NULL,
  baseline_units INT NOT NULL,
  baseline_method VARCHAR(100) NOT NULL,
  baseline_version VARCHAR(50) NOT NULL,
  baseline_run_ts TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);
```

## Reproducibility

* Store the baseline method name and parameters in a metadata table
* Save the code or algorithm reference in a controlled repo and record the commit hash

Example metadata table

```sql
CREATE TABLE baseline_method_registry (
  method_id VARCHAR(100) PRIMARY KEY,
  description VARCHAR(1000),
  parameters VARIANT, -- JSON of parameters
  code_repo_ref VARCHAR(255),
  created_by VARCHAR(100),
  created_ts TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);
```

## Validation

* Keep a snapshot of baseline calculations at each run
* Allow side-by-side comparison between versions

Comparison query example

```sql
SELECT b1.date_key, b1.location_id, b1.product_id,
       b1.baseline_units AS v1, b2.baseline_units AS v2,
       (b2.baseline_units - b1.baseline_units) AS delta
FROM fact_baseline b1
JOIN fact_baseline b2
  ON b1.date_key = b2.date_key
  AND b1.location_id = b2.location_id
  AND b1.product_id = b2.product_id
WHERE b1.baseline_version = 'v1' AND b2.baseline_version = 'v2';
```

## Deliverables

* fact_baseline DDL or added baseline fields in fact table
* baseline_method_registry DDL
* Baseline run snapshot process
* Comparison and validation queries


# Document 5: Campaign Temporal Validation

## Purpose

Ensure campaign-linked facts only reference campaigns that are active and targeted for that geography and product.

## Rules

* Fact.date must fall between Campaign.start_date and Campaign.end_date when campaign_id is not null
* Campaign.target_country must include the fact country or be set to All or equivalent
* A fact must not belong to multiple overlapping campaigns for the same product and location

## Example DDL for Dim Campaign

```sql
CREATE TABLE dim_campaign (
  campaign_id INT PRIMARY KEY,
  campaign_name VARCHAR(255),
  channel VARCHAR(100),
  start_date DATE,
  end_date DATE,
  target_country VARCHAR(100),
  target_product_id INT
);
```

## Validation Queries

Facts outside campaign window

```sql
SELECT f.*
FROM fact_market_sales f
JOIN dim_campaign c ON f.campaign_id = c.campaign_id
WHERE f.date_key < TO_NUMBER(TO_CHAR(c.start_date, 'YYYYMMDD'))
   OR f.date_key > TO_NUMBER(TO_CHAR(c.end_date, 'YYYYMMDD'));
```

Campaign geography mismatch

```sql
SELECT f.*
FROM fact_market_sales f
JOIN dim_location l ON f.location_id = l.location_id
JOIN dim_campaign c ON f.campaign_id = c.campaign_id
WHERE c.target_country <> 'All'
  AND c.target_country <> l.country;
```

Overlapping campaign detection for same product-location-day

```sql
SELECT f.date_key, f.location_id, f.product_id, COUNT(DISTINCT c.campaign_id) AS campaigns
FROM fact_market_sales f
JOIN dim_campaign c
  ON f.product_id = c.target_product_id
  AND f.date_key BETWEEN TO_NUMBER(TO_CHAR(c.start_date,'YYYYMMDD')) AND TO_NUMBER(TO_CHAR(c.end_date,'YYYYMMDD'))
GROUP BY 1,2,3
HAVING COUNT(DISTINCT c.campaign_id) > 1;
```

## Enforcement

* Implement pre-merge checks in ETL that reject rows failing validation
* Create exception tables for manual review and correction

## Deliverables

* Validation SQL scripts
* Pre-merge check template
* Exception reporting table DDL


# Document 6: Snapshot and Accumulation Facts

## Purpose

Provide patterns for snapshot and accumulation facts that simplify executive and campaign lifecycle reporting.

## Options

1. Campaign Daily Snapshot Fact

* One row per campaign per day with cumulative metrics

2. Campaign Lifecycle Accumulation Fact

* One row per campaign with aggregated start-to-end metrics

## Example DDL: Campaign Daily Snapshot

```sql
CREATE TABLE fact_campaign_snapshot (
  campaign_id INT,
  date_key INT,
  daily_actual_units INT,
  daily_baseline_units INT,
  daily_lift_units INT,
  cumulative_actual_units INT,
  cumulative_baseline_units INT,
  cumulative_lift_units INT,
  snapshot_run_ts TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (campaign_id, date_key)
);
```

## Population strategy

* Daily ETL job that computes daily metrics and maintains running totals using window functions

Population example

```sql
INSERT INTO fact_campaign_snapshot
SELECT c.campaign_id,
       f.date_key,
       SUM(f.actual_units) AS daily_actual_units,
       SUM(f.baseline_units) AS daily_baseline_units,
       SUM(f.actual_units) - SUM(f.baseline_units) AS daily_lift_units,
       SUM(SUM(f.actual_units)) OVER (PARTITION BY c.campaign_id ORDER BY f.date_key ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_actual_units,
       SUM(SUM(f.baseline_units)) OVER (PARTITION BY c.campaign_id ORDER BY f.date_key ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_baseline_units,
       SUM(SUM(f.actual_units) - SUM(f.baseline_units)) OVER (PARTITION BY c.campaign_id ORDER BY f.date_key ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_lift_units,
       CURRENT_TIMESTAMP
FROM fact_market_sales f
JOIN dim_campaign c ON f.campaign_id = c.campaign_id
GROUP BY c.campaign_id, f.date_key;
```

## Deliverables

* Snapshot fact DDL
* ETL job template for daily refresh
* Backfill strategy for historical campaigns


# Document 7: Data Quality SQL Checks

## Purpose

Provide a repeatable suite of SQL checks to validate data health for fact and dimensions.

## Categories of checks

* Completeness
* Referential integrity
* Range and sanity checks
* Uniqueness
* Anomaly detection

## Example Checks

Completeness

```sql
SELECT COUNT(*) AS missing_baseline
FROM fact_market_sales
WHERE baseline_units IS NULL;
```

Referential integrity

```sql
SELECT f.*
FROM fact_market_sales f
LEFT JOIN dim_product p ON f.product_id = p.product_id
WHERE p.product_id IS NULL;
```

Range sanity

```sql
SELECT * FROM fact_market_sales WHERE baseline_units < 0 OR actual_units < 0;
```

Lift sanity

```sql
SELECT * FROM fact_market_sales WHERE lift_units > 100000 OR lift_units < -100000;
```

Uniqueness

```sql
-- refer to Document 1 duplicate detection
```

Anomaly detection

* Use simple z-score or percentage change checks over rolling windows to detect sudden spikes

## Automation and Reporting

* Store check results in a data_quality_results table with check_name, severity, row_count, sample_rows, run_ts
* Build a simple dashboard that surfaces failing checks and recent trends

## Deliverables

* SQL check suite
* data_quality_results DDL
* Alert thresholds and run schedule


# Document 8: Semantic Layer Views

## Purpose

Provide certified SQL views for BI consumption that centralize business logic and prevent metric drift.

## Principles

* Views must expose clearly named, versioned metrics
* Avoid complex business logic in BI tool calculations
* Provide both grain-level views and aggregated convenience views

## Example Certified View: market_sales_vw

```sql
CREATE OR REPLACE VIEW market_sales_vw AS
SELECT f.date_key,
       f.location_id,
       l.country,
       f.product_id,
       p.brand_name,
       f.campaign_id,
       COALESCE(f.baseline_units,0) AS baseline_units,
       COALESCE(f.actual_units,0) AS actual_units,
       COALESCE(f.actual_units,0) - COALESCE(f.baseline_units,0) AS lift_units,
       f.revenue_usd
FROM fact_market_sales f
JOIN dim_product p ON f.product_id = p.product_id
JOIN dim_location l ON f.location_id = l.location_id;
```

## Aggregated convenience view

```sql
CREATE OR REPLACE VIEW campaign_performance_agg_vw AS
SELECT campaign_id,
       SUM(actual_units) AS total_actual_units,
       SUM(baseline_units) AS total_baseline_units,
       SUM(actual_units) - SUM(baseline_units) AS total_lift_units,
       SUM(revenue_usd) AS total_revenue
FROM market_sales_vw
GROUP BY 1;
```

## Governance

* Tag certified views with owner and version metadata
* Maintain view change log

## Deliverables

* market_sales_vw DDL
* campaign_performance_agg_vw DDL
* View ownership matrix


# Document 9: Performance Optimization Strategy

## Purpose

Provide database and SQL patterns to ensure the warehouse performs well at scale.

## Key Strategies

* Partition or cluster large fact tables by date_key
* Materialize heavy aggregates as summary tables
* Limit the use of wide joins in dashboard queries by using certified views
* Use query profiling to identify hotspots

## Example: Partitioning and Clustering

* If platform supports partitioning, partition by date_key and cluster by (product_id, location_id)

Example Snowflake clustering

```sql
ALTER TABLE fact_market_sales CLUSTER BY (date_key, product_id, location_id);
```

## Aggregate tables

* Build monthly and weekly aggregates for common executive queries

Monthly aggregate DDL example

```sql
CREATE TABLE agg_monthly_sales AS
SELECT product_id, location_id, TO_NUMBER(TO_CHAR(TO_DATE(CAST(date_key AS VARCHAR), 'YYYYMMDD'), 'YYYYMM')) AS month_key,
       SUM(actual_units) AS actual_units,
       SUM(baseline_units) AS baseline_units
FROM fact_market_sales
GROUP BY 1,2,3;
```

## Query best practices

* Push filters early using WHERE on date_key
* Prefer joins on surrogate keys
* Avoid SELECT * in production dashboards

## Monitoring

* Capture query history and surface top expensive queries weekly
* Track cardinality growth of dimensions and adjust clustering keys

## Deliverables

* Clustering / partitioning recommendations per platform
* Aggregate table catalog and refresh schedule
* Query profiling and monitoring queries


# Appendix: Common SQL Templates

* Dedup MERGE template
* SCD Type 2 MERGE template
* Data quality checks runner
* Snapshot population query

Copy these templates into your ETL jobs and adapt to your platform's SQL dialect.


End of documents